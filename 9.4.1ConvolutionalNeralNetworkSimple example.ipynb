{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n",
    "<h1 align=center><font size = 5>Convolutional Neral Network Simple example </font></h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Objective for this Notebook<h3>    \n",
    "<h5> 1. Learn Convolutional Neral Network</h5>\n",
    "<h5> 2. Define Softmax , Criterion function, Optimizer and Train the  Model</h5>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines \n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<li><a href=\"#ref0\">Helper functions </a></li>\n",
    "\n",
    "<li><a href=\"#ref1\"> Prepare Data </a></li>\n",
    "<li><a href=\"#ref2\">Convolutional Neral Network </a></li>\n",
    "<li><a href=\"#ref3\">Define Softmax , Criterion function, Optimizer and Train the  Model</a></li>\n",
    "<li><a href=\"#ref4\">Analyse Results</a></li>\n",
    "\n",
    "<br>\n",
    "<p></p>\n",
    "Estimated Time Needed: <strong>25 min</strong>\n",
    "</div>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref0\"></a>\n",
    "<h2 align=center>Helper functions </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa5880ff0b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to plot out the parameters of the Convolutional layers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_channels(W):\n",
    "    #number of output channels \n",
    "    n_out=W.shape[0]\n",
    "    #number of input channels \n",
    "    n_in=W.shape[1]\n",
    "    w_min=W.min().item()\n",
    "    w_max=W.max().item()\n",
    "    fig, axes = plt.subplots(n_out,n_in)\n",
    "    fig.subplots_adjust(hspace = 0.1)\n",
    "    out_index=0\n",
    "    in_index=0\n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "    \n",
    "        if in_index>n_in-1:\n",
    "            out_index=out_index+1\n",
    "            in_index=0\n",
    "              \n",
    "        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index=in_index+1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>show_data</code>: plot out data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(dataset,sample):\n",
    "\n",
    "    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n",
    "    plt.title('y='+str(dataset.y[sample].item()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some toy data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n",
    "        \"\"\"\n",
    "        p:portability that pixel is wight  \n",
    "        N_images:number of images \n",
    "        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n",
    "        \"\"\"\n",
    "        if train==True:\n",
    "            np.random.seed(1)  \n",
    "        \n",
    "        #make images multiple of 3 \n",
    "        N_images=2*(N_images//2)\n",
    "        images=np.zeros((N_images,1,11,11))\n",
    "        start1=3\n",
    "        start2=1\n",
    "        self.y=torch.zeros(N_images).type(torch.long)\n",
    "\n",
    "        for n in range(N_images):\n",
    "            if offset>0:\n",
    "        \n",
    "                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n",
    "                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n",
    "            else:\n",
    "                low=4\n",
    "                high=1\n",
    "        \n",
    "            if n<=N_images//2:\n",
    "                self.y[n]=0\n",
    "                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n",
    "            elif  n>N_images//2:\n",
    "                self.y[n]=1\n",
    "                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n",
    "           \n",
    "        \n",
    "        \n",
    "        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        self.len=self.x.shape[0]\n",
    "        del(images)\n",
    "        np.random.seed(0)\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>plot_activation</code>: plot out the activations of the Convolutional layers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(A,number_rows= 1,name=\"\"):\n",
    "    A=A[0,:,:,:].detach().numpy()\n",
    "    n_activations=A.shape[0]\n",
    "    \n",
    "    \n",
    "    print(n_activations)\n",
    "    A_min=A.min().item()\n",
    "    A_max=A.max().item()\n",
    "\n",
    "    if n_activations==1:\n",
    "\n",
    "        # Plot the image.\n",
    "        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n",
    "        fig.subplots_adjust(hspace = 0.4)\n",
    "        for i,ax in enumerate(axes.flat):\n",
    "            if i< n_activations:\n",
    "                # Set the label for the sub-plot.\n",
    "                ax.set_xlabel( \"activation:{0}\".format(i+1))\n",
    "\n",
    "                # Plot the image.\n",
    "                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    #by Duane Nielsen\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "<h2 align=center>Prepare Data </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset with 10000 samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_images=10000\n",
    "train_dataset=Data(N_images=N_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Data at 0x7fa50cf805d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset=Data(N_images=1000,train=False)\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the data type is long \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the third label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the 3rd  sample \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "### Build a Convolutional Neral Network Class \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input image is 11 x11, the following will change the size of the activations:\n",
    "<ul>\n",
    "<il>convolutional layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>convolutional layer </il>\n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer </il>\n",
    "</ul>\n",
    "\n",
    "with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n",
    "We use the following  lines of code to change the image before we get tot he fully connected layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(9, 9)\n",
      "(8, 8)\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out)\n",
    "out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out1)\n",
    "out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out2)\n",
    "\n",
    "out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,out_1=2,out_2=1):\n",
    "        \n",
    "        super(CNN,self).__init__()\n",
    "        #first Convolutional layers \n",
    "        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "\n",
    "        #second Convolutional layers\n",
    "        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "        #max pooling \n",
    "\n",
    "        #fully connected layer \n",
    "        self.fc1=nn.Linear(out_2*7*7,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn1(x)\n",
    "        #activation function \n",
    "        x=torch.relu(x)\n",
    "        #max pooling \n",
    "        x=self.maxpool1(x)\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn2(x)\n",
    "        #activation function\n",
    "        x=torch.relu(x)\n",
    "        #max pooling\n",
    "        x=self.maxpool2(x)\n",
    "        #flatten output \n",
    "        x=x.view(x.size(0),-1)\n",
    "        #fully connected layer\n",
    "        x=self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def activations(self,x):\n",
    "        #outputs activation this is not necessary just for fun \n",
    "        z1=self.cnn1(x)\n",
    "        a1=torch.relu(z1)\n",
    "        out=self.maxpool1(a1)\n",
    "        \n",
    "        z2=self.cnn2(out)\n",
    "        a2=torch.relu(z2)\n",
    "        out=self.maxpool2(a2)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        return z1,a1,z2,a2,out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "<h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 output channels for the first layer, and 1 outputs channel for the second layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNN(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the model parameters with the object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn1): Conv2d(1, 2, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(2, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=49, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAAGKCAYAAABJvw5NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAID0lEQVR4nO3bQYobVx7H8b+azrzgQMlgZhaGam+zyyFmMTDrQA6QSyQr4ZUuMqshF5gjzCG8sfZ2l3rwUFm4ZuFYi7j754qnC6nVnw88jOSH+NP0l1cPWqtpmqYCbnVx7AHglAkEAoFAIBAIBAKBQCAQCAQCgeBy7sZxHGscx8Pr9+/f15s3b+rZs2e1Wq0WGQ6WMk1T3dzc1PPnz+viIpwT00ybzWaqKss6q7Xb7eLv/Wrun5r8/gQZhqGurq5q98sv1T15Mucj4GTs372r/vvv6/r6utbr9Z37Zj9itdaqtfbJ+92TJ9V9882XTQlH9rnrgUs6BAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAILicu3EcxxrH8fB6v98vMhCcktknyHa7rfV6fVh93y85F5yE2YH8/PPPNQzDYe12uyXngpMw+xGrtVattSVngZPjkg6BQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAILuduHMexxnE8vN7v94sMBKdkdiDb7bZevnz5yfvrv/+7qr6+z5k4+OHYA5yxm1m7VtM0TXM23naC9H1fVZsSyFIEspybqvquhmGoruvu3DX7BGmtVWvtPiaDB8MlHQKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBBczt04jmON43h4vd/vFxkITsnsE2S73dZ6vT6svu+XnAtOwmqapmnOxttOkA+RbKrq64XGe+x+OPYAZ+ymqr6rYRiq67o7d81+xGqtVWvtPiaDB8MlHQKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBBczt04jmON43h4PQzDx/+575k4uDn2AGfsP1VVNU1T3jbNtNlspqqyrLNar169ir/3q+mzCX3w+xPk+vq6Xrx4Ua9fv671ej3nI45qv99X3/e12+2q67pjj/NZ5l3WMAx1dXVVb9++radPn965b/YjVmutWmufvL9erx/ED+SjruvMu6CHNu/FRb6Gu6RDIBAIvjiQ1lptNptbH7tOkXmXda7zzr6kw2PkEQsCgUAgEAgEAoFAIBAIBAKBQCAQCASCL/7C1Pv37+vNmzf17NmzWq1WiwwHS5mmqW5ubur58+f5L3p9Ycp6zGu32y3zhamPXzip+mv9gYOIP+TPxx7gjP1aVf+s6+vr+IW///sLUx8+4qs/Ph8z/OnYA5y9z10PXNIhEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCweXcjeM41jiOh9f7/X6RgeCUzD5Bttttrdfrw+r7fsm54CSspmma5my87QT5EMnfquqrhcZ77P5y7AHO2K9V9Y8ahqG6rrtz1+xHrNZatdbuYzJ4MFzSIRAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAsHl3I3jONY4jofX+/1+kYHglMwOZLvd1suXLz95/6f6V7V7HYmPvj32AGfsXVX9OGPfapqmac4H3naC9H1fP1UJZCECWc7HQIZhqK7r7tw3+wRprVVrUuBxcUmHQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBJdzN47jWOM4Hl7v9/tFBoJTMvsE2W63tV6vD6vv+yXngpOwmqZpmrPxthOk7/v6qaraUtM9ct8ee4Az9q6qfqyqYRiq67o7981+xGqtVWtS4HFxSYdAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEAoFAIBAIBAKBQCAQCAQCgUAgEAgEl3M3juNY4zgeXg/D8OH9+5+J37w79gBn7L+//TtNU944zbTZbKaqsqyzWq9evYq/96vpswl98PsT5Pr6ul68eFGvX7+u9Xo95yOOar/fV9/3tdvtquu6Y4/zWeZd1jAMdXV1VW/fvq2nT5/euW/2I1ZrrVprn7y/Xq8fxA/ko67rzLughzbvxUW+hrukQyAQCL44kNZabTabWx+7TpF5l3Wu886+pMNj5BELAoFAIBAIBAKBQCAQCAQCgUAgEPwP7ouF1+HbSwIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_channels(model.state_dict()['cnn1.weight'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " optimizer class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "N_test=len(validation_dataset)\n",
    "cost=0\n",
    "#n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    cost=0    \n",
    "    for x, y in train_loader:\n",
    "      \n",
    "\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z=model(x)\n",
    "        # calculate loss \n",
    "        loss=criterion(z,y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        cost+=loss.item()\n",
    "    cost_list.append(cost)\n",
    "        \n",
    "        \n",
    "    correct=0\n",
    "    #perform a prediction on the validation  data  \n",
    "    for x_test, y_test in validation_loader:\n",
    "\n",
    "        z=model(x_test)\n",
    "        _,yhat=torch.max(z.data,1)\n",
    "\n",
    "        correct+=(yhat==y_test).sum().item()\n",
    "        \n",
    "\n",
    "    accuracy=correct/N_test\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"ref3\"></a>\n",
    "<h2 align=center>Analyse Results</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and accuracy on the validation data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(cost_list,color=color)\n",
    "ax1.set_xlabel('epoch',color=color)\n",
    "ax1.set_ylabel('total loss',color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  \n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results of the parameters for the Convolutional layers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the activations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\n",
    "out=model.activations(train_dataset[0][0].view(1,1,11,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot them out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[0],number_rows=1,name=\" feature map\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[3],number_rows=1,name=\"first feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we save the output of the activation after flattening  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=out[4][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do the same for a sample  where y=0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot( out1, 'b')\n",
    "plt.title('Flatted Activation Values  ')\n",
    "plt.ylabel('Activation')\n",
    "plt.xlabel('index')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(out0, 'r')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Authors:  \n",
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n",
    "\n",
    "Other contributors: [Michelle Carey](https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> Â© IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
